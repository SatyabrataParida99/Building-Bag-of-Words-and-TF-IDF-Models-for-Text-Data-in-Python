{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b447efb-0c23-47b9-8c77-29f2dc059671",
   "metadata": {},
   "source": [
    "$$Bag of Words (BoW) & TF-IDF (Term Frequency-Inverse Document Frequency)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe63f82f-f642-4c4e-b1ed-d3fa2d00788c",
   "metadata": {},
   "source": [
    "**Bag of Words (BoW)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76af6243-2916-42e0-9725-45088ae6e808",
   "metadata": {},
   "source": [
    "- Bag of Words (BoW) in NLP is a method to represent text as numerical data by creating a vocabulary of unique words from the dataset and counting their occurrences in each document. It ignores grammar, word order, and context, focusing only on word frequency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032b5738-836f-4119-9c34-6114528c56e7",
   "metadata": {},
   "source": [
    "<img src=\"https://miro.medium.com/v2/resize:fit:893/1*axffCQ9ae0FHXxhuy66FbA.png\" jsaction=\"\" class=\"sFlh5c FyHeAf iPVvYb\" style=\"max-width: 893px; height: 152px; margin: 0px; width: 351px;\" alt=\"4 — Bag of Words Model in NLP. In this article, we will cover the Bag… | by  Aysel Aydin | Medium\" jsname=\"kn3ccd\" aria-hidden=\"false\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04510bab-6dde-4683-98e9-0adb1336ff1f",
   "metadata": {},
   "source": [
    "<img src=\"https://i.postimg.cc/pLvhy7zs/basicbow.png\" jsaction=\"\" class=\"sFlh5c FyHeAf iPVvYb\" style=\"max-width: 880px; height: 156px; margin: 0.5px 0px; width: 351px;\" alt=\"NLP - Bag of Words (BOW)\" jsname=\"kn3ccd\" aria-hidden=\"false\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13f3193-5f82-4fe7-a564-9ffa48afd111",
   "metadata": {},
   "source": [
    "**TF-IDF (Term Frequency-Inverse Document Frequency)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a3037c-f39a-41ab-98d3-e1f4c3579742",
   "metadata": {},
   "source": [
    "- TF-IDF (Term Frequency-Inverse Document Frequency) is a statistical method used in NLP to determine the importance of a word in a document relative to a collection of documents (corpus). It assigns higher weights to words that are frequent in a specific document but rare across the corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92180c7f-68b4-4395-987f-495cecd873e8",
   "metadata": {},
   "source": [
    "<img src=\"https://assets.zilliz.com/TF_IDF_Understanding_Term_Frequency_Inverse_Document_Frequency_in_NLP_04d3c51de7.png\" jsaction=\"\" class=\"sFlh5c FyHeAf iPVvYb\" style=\"max-width: 2400px; height: 184px; margin: 0.5px 0px; width: 351px;\" alt=\"TF-IDF - Understanding Term Frequency-Inverse Document Frequency in NLP -  Zilliz Learn\" jsname=\"kn3ccd\" aria-hidden=\"false\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5842a28a-c2a3-4501-abb3-dfc14b293158",
   "metadata": {},
   "source": [
    "**Step 1: Importing Required Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a338143f-6c94-4bb4-81df-b06f7796ce33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk: For natural language processing tasks\n",
    "# re: For regular expressions to clean text\n",
    "# stopwords: To remove commonly used words that do not contribute to meaning\n",
    "# PorterStemmer, WordNetLemmatizer: For stemming and lemmatization\n",
    "\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1ad282-2959-40f9-b428-5086489041ca",
   "metadata": {},
   "source": [
    "**Step 2: Paragraph Input**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "afe053aa-3584-40f2-b8fc-834ab1a61505",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text data for preprocessing and feature extraction\n",
    "\n",
    "paragraph = \"\"\"Over the span of 3000 years, India has been a land where diverse cultures, ideas, and forces from around the world have interacted with us. We've experienced invasions, territorial conquests, and profound shifts in our ways of thinking. Today, I envision a future where India stands united, empowered by its rich heritage and ready to face the challenges of tomorrow....\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3efa1846-fd6c-4d27-9f3d-425a9e506e83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Over the span of 3000 years, India has been a land where diverse cultures, ideas, and forces from around the world have interacted with us. We've experienced invasions, territorial conquests, and profound shifts in our ways of thinking. Today, I envision a future where India stands united, empowered by its rich heritage and ready to face the challenges of tomorrow....\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paragraph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f93987-7d25-4436-954c-ce964a4448da",
   "metadata": {},
   "source": [
    "**Step 3: Tokenization (Splitting paragraph into sentences)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "50fd1a5c-9ad3-40da-b613-a036dd316e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = nltk.sent_tokenize(paragraph)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2eb66b1-4df7-41d8-a4f9-9f569931e362",
   "metadata": {},
   "source": [
    "**Step 4: Initializing Tools for Cleaning**\n",
    "- PorterStemmer for stemming (not used in this implementation)\n",
    "- WordNetLemmatizer for lemmatization (used in this implementation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c79b03c3-78ea-46fb-8079-794542083352",
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()\n",
    "wordnet = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33acb67e-62e3-4f5a-801a-923d7c12e3e9",
   "metadata": {},
   "source": [
    "**Step 5: Text Cleaning and Preprocessing**\n",
    "- Create an empty list to store the cleaned sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7ebed54f-ac9a-47ce-9b6d-b495e23cef50",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b478edf6-7840-4990-9636-a5fbf8bbf6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(sentences)):\n",
    "    # Remove all non-alphabet characters using regular expressions\n",
    "    review = re.sub('[^a-zA-Z]', ' ', sentences[i])\n",
    "    review = review.lower()  # Convert text to lowercase\n",
    "    review = review.split()  # Split sentence into words\n",
    "    \n",
    "    # Remove stopwords and apply lemmatization\n",
    "    review = [wordnet.lemmatize(word) for word in review if not word in set(stopwords.words('english'))]\n",
    "    \n",
    "    # Join the cleaned words back into a sentence\n",
    "    review = ' '.join(review)\n",
    "    corpus.append(review)  # Append the cleaned sentence to the corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f7af55-21b6-4428-ad88-2ccb19cb7f14",
   "metadata": {},
   "source": [
    "**Step 6: Creating the Bag of Words (BoW) Model**\n",
    "- Importing CountVectorizer to create the BoW representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0eb39709-8490-49f7-ad07-640c70d9149a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv_bow = CountVectorizer()  # Initialize CountVectorizer\n",
    "X_bow = cv_bow.fit_transform(corpus).toarray()  # Fit and transform the cleaned data into a matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf9a926-02da-4a30-9d26-d31db452a64c",
   "metadata": {},
   "source": [
    "**Step 7: Creating the TF-IDF Model**\n",
    "- Importing TfidfVectorizer to create the TF-IDF representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "723f56b9-a62f-41bf-acd2-6e03c6936a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "cv_tfidf = TfidfVectorizer()  # Initialize TfidfVectorizer\n",
    "X_tfidf = cv_tfidf.fit_transform(corpus).toarray()  # Fit and transform the cleaned data into a matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d5fff1-a458-4bc4-a0ef-5f38cd44029d",
   "metadata": {},
   "source": [
    "**Step 8: Displaying Results**\n",
    "- Display the feature matrix for Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f740a766-624b-4c54-b2ac-8a4f89eef9ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Bag of Words Matrix:\n",
      "[[1 0 0 1 1 0 0 0 0 1 0 0 1 1 1 0 1 0 0 0 0 1 0 0 0 0 0 0 0 1 1]\n",
      " [0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 1 0 1 0 0 1 0 0 1 1 0 0 0 1 0 0]\n",
      " [0 1 0 0 0 1 1 0 1 0 1 1 0 1 0 0 0 0 1 1 0 0 1 0 0 1 1 1 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nBag of Words Matrix:\")\n",
    "print(X_bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6c2151-42f3-4d5f-839a-63e1ce99a237",
   "metadata": {},
   "source": [
    "**Display the feature matrix for TF-IDF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dd2c3aaa-9ec4-41cf-a569-27736e00d6cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TF-IDF Matrix:\n",
      "[[0.30746099 0.         0.         0.30746099 0.30746099 0.\n",
      "  0.         0.         0.         0.30746099 0.         0.\n",
      "  0.30746099 0.23383201 0.30746099 0.         0.30746099 0.\n",
      "  0.         0.         0.         0.30746099 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.30746099\n",
      "  0.30746099]\n",
      " [0.         0.         0.35355339 0.         0.         0.\n",
      "  0.         0.35355339 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.35355339 0.         0.35355339\n",
      "  0.         0.         0.35355339 0.         0.         0.35355339\n",
      "  0.35355339 0.         0.         0.         0.35355339 0.\n",
      "  0.        ]\n",
      " [0.         0.28195987 0.         0.         0.         0.28195987\n",
      "  0.28195987 0.         0.28195987 0.         0.28195987 0.28195987\n",
      "  0.         0.21443775 0.         0.         0.         0.\n",
      "  0.28195987 0.28195987 0.         0.         0.28195987 0.\n",
      "  0.         0.28195987 0.28195987 0.28195987 0.         0.\n",
      "  0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTF-IDF Matrix:\")\n",
    "print(X_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8b2b7d-9884-4ac3-8b9f-79dfbe7e7274",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
